{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0517ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape comments for 特朗普关税王八拳，打中国，打美元，国际贸易必然重塑_哔哩哔哩_bilibili...\n",
      "Scraping complete! Results saved as 特朗普关税王八拳，打中国，打美元，国际贸_评论.csv\n"
     ]
    }
   ],
   "source": [
    "#Video 1. Trump’s Tariff King Punches: Hitting China, the U.S. Dollar, and America — A Reshaping of Global Trade Is Inevitable\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Securely read cookies ===\n",
    "def get_header(cookie_path='bili_cookie1.txt'):\n",
    "    if not os.path.exists(cookie_path):\n",
    "        raise FileNotFoundError(f\"Cookie file '{cookie_path}' not found. Please check the path.\")\n",
    "    with open(cookie_path, 'r', encoding='utf-8') as f:\n",
    "        cookie = f.read().strip()\n",
    "    headers = {\n",
    "        \"Cookie\": cookie,\n",
    "        \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# === 2. Retrieve video information (OID + title) ===\n",
    "def get_video_info(bv):\n",
    "    url = f\"https://www.bilibili.com/video/{bv}\"\n",
    "    resp = requests.get(url, headers=get_header())\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text\n",
    "    \n",
    "    oid_match = re.search(r'\"aid\":(\\d+),\"bvid\":\"{}\"'.format(bv), text)\n",
    "    title_match = re.search(r'<title.*?>(.*?)</title>', text)\n",
    "    \n",
    "    if not oid_match or not title_match:\n",
    "        raise ValueError(\"Failed to extract OID or Title from the video page.\")\n",
    "    \n",
    "    oid = oid_match.group(1)\n",
    "    title = title_match.group(1).strip().replace('/', '_')  # 防止文件名非法字符\n",
    "    return oid, title\n",
    "\n",
    "# === 3. Scrape comments ===\n",
    "def crawl_comments(bv, oid, csv_writer, is_second=True, page_cursor=''):\n",
    "    mode, plat, type_, web_location = 2, 1, 1, 1315875\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    pagination = {\"offset\": \"\"} if not page_cursor else {\"offset\": json.dumps({\"type\": 3, \"direction\": 1, \"Data\": {\"cursor\": page_cursor}})}\n",
    "    pagination_str = json.dumps(pagination, separators=(',', ':'))\n",
    "    \n",
    "    # Generate w_rid\n",
    "    params = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str)}&plat={plat}&seek_rpid=&type={type_}&web_location={web_location}&wts={timestamp}ea1db124af3c7062474693fa704f4ff8\"\n",
    "    w_rid = hashlib.md5(params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    url = (\n",
    "        f\"https://api.bilibili.com/x/v2/reply/wbi/main?\"\n",
    "        f\"oid={oid}&type={type_}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}\"\n",
    "        f\"&plat={plat}&seek_rpid=&web_location={web_location}&w_rid={w_rid}&wts={timestamp}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(url, headers=get_header())\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    replies = data.get('data', {}).get('replies', [])\n",
    "    if not replies:\n",
    "        return None  # No more replies\n",
    "\n",
    "    count = 0\n",
    "    for reply in replies:\n",
    "        count += 1\n",
    "        write_comment(reply, csv_writer)\n",
    "        \n",
    "        # If secondary (nested) comments are enabled\n",
    "        if is_second and reply.get('rcount', 0) > 0:\n",
    "            crawl_sub_comments(oid, reply['rpid'], csv_writer)\n",
    "    \n",
    "    next_cursor = data['data']['cursor']['next']\n",
    "    return next_cursor if next_cursor != 0 else None\n",
    "\n",
    "# === 4. Scrape secondary (nested) comments ===\n",
    "def crawl_sub_comments(oid, root_rpid, csv_writer):\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/reply?oid={oid}&type=1&root={root_rpid}&ps=10&pn={page}&web_location=333.788\"\n",
    "        response = requests.get(url, headers=get_header())\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        replies = data.get('data', {}).get('replies', [])\n",
    "        if not replies:\n",
    "            break\n",
    "        \n",
    "        for reply in replies:\n",
    "            write_comment(reply, csv_writer)\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.2)  # Avoid getting banned by scraping too quickly\n",
    "\n",
    "# === 5. Write a single comment to CSV ===\n",
    "def write_comment(reply, csv_writer):\n",
    "    try:\n",
    "        csv_writer.writerow([\n",
    "            reply.get('parent', ''),\n",
    "            reply.get('rpid', ''),\n",
    "            reply.get('mid', ''),\n",
    "            reply['member'].get('uname', ''),\n",
    "            reply['member']['level_info'].get('current_level', ''),\n",
    "            reply['member'].get('sex', ''),\n",
    "            reply['content'].get('message', '').replace('\\n', ' '),\n",
    "            pd.to_datetime(reply['ctime'], unit='s'),\n",
    "            reply['reply_control'].get('sub_reply_entry_text', '0').strip(),\n",
    "            reply.get('like', 0),\n",
    "            reply['member'].get('sign', ''),\n",
    "            reply['reply_control'].get('location', '未知')[5:] if 'location' in reply['reply_control'] else '未知',\n",
    "            '是' if reply['member']['vip'].get('vipStatus', 0) != 0 else '否',\n",
    "            reply['member'].get('avatar', '')\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"写入评论出错: {e}\")\n",
    "\n",
    "# ===6. Main program entry point ===\n",
    "def main():\n",
    "    bv = input(\"请输入B站BV号（如BV1CDdWYHEtU）: \").strip()\n",
    "    oid, title = get_video_info(bv)\n",
    "    output_file = f\"{title[:20]}_评论.csv\"\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['上级评论ID', '评论ID', '用户ID', '用户名', '用户等级', '性别', '评论内容', '评论时间', '回复数', '点赞数', '个性签名', 'IP属地', '是否是大会员', '头像'])\n",
    "        \n",
    "        print(f\"Starting to scrape comments for {title}...\")\n",
    "        page_cursor = ''\n",
    "        while page_cursor is not None:\n",
    "            page_cursor = crawl_comments(bv, oid, writer, is_second=True, page_cursor=page_cursor)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(f\"Scraping complete! Results saved as {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30cddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape comments for 【爆笑每日秀】美国人吐槽特朗普加征关税，全程高能讽刺_哔哩哔哩_bilibili...\n",
      "Scraping complete! Results saved as 【爆笑每日秀】美国人吐槽特朗普加征关税，_评论.csv\n"
     ]
    }
   ],
   "source": [
    "#Video 2: [Hilarious Daily Show] Americans Roast Trump for Raising Tariffs — Packed with High-Energy Sarcasm\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Securely read cookies ===\n",
    "def get_header(cookie_path='bili_cookie2.txt'):\n",
    "    if not os.path.exists(cookie_path):\n",
    "        raise FileNotFoundError(f\"Cookie file '{cookie_path}' not found. Please check the path.\")\n",
    "    with open(cookie_path, 'r', encoding='utf-8') as f:\n",
    "        cookie = f.read().strip()\n",
    "    headers = {\n",
    "        \"Cookie\": cookie,\n",
    "        \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# === 2. Retrieve video information (OID + title) ===\n",
    "def get_video_info(bv):\n",
    "    url = f\"https://www.bilibili.com/video/{bv}\"\n",
    "    resp = requests.get(url, headers=get_header())\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text\n",
    "    \n",
    "    oid_match = re.search(r'\"aid\":(\\d+),\"bvid\":\"{}\"'.format(bv), text)\n",
    "    title_match = re.search(r'<title.*?>(.*?)</title>', text)\n",
    "    \n",
    "    if not oid_match or not title_match:\n",
    "        raise ValueError(\"Failed to extract OID or Title from the video page.\")\n",
    "    \n",
    "    oid = oid_match.group(1)\n",
    "    title = title_match.group(1).strip().replace('/', '_')  # 防止文件名非法字符\n",
    "    return oid, title\n",
    "\n",
    "# === 3. Scrape comments ===\n",
    "def crawl_comments(bv, oid, csv_writer, is_second=True, page_cursor=''):\n",
    "    mode, plat, type_, web_location = 2, 1, 1, 1315875\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    pagination = {\"offset\": \"\"} if not page_cursor else {\"offset\": json.dumps({\"type\": 3, \"direction\": 1, \"Data\": {\"cursor\": page_cursor}})}\n",
    "    pagination_str = json.dumps(pagination, separators=(',', ':'))\n",
    "    \n",
    "    # Generate w_rid\n",
    "    params = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str)}&plat={plat}&seek_rpid=&type={type_}&web_location={web_location}&wts={timestamp}ea1db124af3c7062474693fa704f4ff8\"\n",
    "    w_rid = hashlib.md5(params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    url = (\n",
    "        f\"https://api.bilibili.com/x/v2/reply/wbi/main?\"\n",
    "        f\"oid={oid}&type={type_}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}\"\n",
    "        f\"&plat={plat}&seek_rpid=&web_location={web_location}&w_rid={w_rid}&wts={timestamp}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(url, headers=get_header())\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    replies = data.get('data', {}).get('replies', [])\n",
    "    if not replies:\n",
    "        return None  # No more replies\n",
    "\n",
    "    count = 0\n",
    "    for reply in replies:\n",
    "        count += 1\n",
    "        write_comment(reply, csv_writer)\n",
    "        \n",
    "        # If secondary (nested) comments are enabled\n",
    "        if is_second and reply.get('rcount', 0) > 0:\n",
    "            crawl_sub_comments(oid, reply['rpid'], csv_writer)\n",
    "    \n",
    "    next_cursor = data['data']['cursor']['next']\n",
    "    return next_cursor if next_cursor != 0 else None\n",
    "\n",
    "# === 4. Scrape secondary (nested) comments ===\n",
    "def crawl_sub_comments(oid, root_rpid, csv_writer):\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/reply?oid={oid}&type=1&root={root_rpid}&ps=10&pn={page}&web_location=333.788\"\n",
    "        response = requests.get(url, headers=get_header())\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        replies = data.get('data', {}).get('replies', [])\n",
    "        if not replies:\n",
    "            break\n",
    "        \n",
    "        for reply in replies:\n",
    "            write_comment(reply, csv_writer)\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.2)  # Avoid getting banned by scraping too quickly\n",
    "\n",
    "# === 5. Write a single comment to CSV ===\n",
    "def write_comment(reply, csv_writer):\n",
    "    try:\n",
    "        csv_writer.writerow([\n",
    "            reply.get('parent', ''),\n",
    "            reply.get('rpid', ''),\n",
    "            reply.get('mid', ''),\n",
    "            reply['member'].get('uname', ''),\n",
    "            reply['member']['level_info'].get('current_level', ''),\n",
    "            reply['member'].get('sex', ''),\n",
    "            reply['content'].get('message', '').replace('\\n', ' '),\n",
    "            pd.to_datetime(reply['ctime'], unit='s'),\n",
    "            reply['reply_control'].get('sub_reply_entry_text', '0').strip(),\n",
    "            reply.get('like', 0),\n",
    "            reply['member'].get('sign', ''),\n",
    "            reply['reply_control'].get('location', '未知')[5:] if 'location' in reply['reply_control'] else '未知',\n",
    "            '是' if reply['member']['vip'].get('vipStatus', 0) != 0 else '否',\n",
    "            reply['member'].get('avatar', '')\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"写入评论出错: {e}\")\n",
    "\n",
    "# === 6. Main program entry point ===\n",
    "def main():\n",
    "    bv = input(\"请输入B站BV号（如BV1CDdWYHEtU）: \").strip()\n",
    "    oid, title = get_video_info(bv)\n",
    "    output_file = f\"{title[:20]}_评论.csv\"\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['上级评论ID', '评论ID', '用户ID', '用户名', '用户等级', '性别', '评论内容', '评论时间', '回复数', '点赞数', '个性签名', 'IP属地', '是否是大会员', '头像'])\n",
    "        \n",
    "        print(f\"Starting to scrape comments for {title}...\")\n",
    "        page_cursor = ''\n",
    "        while page_cursor is not None:\n",
    "            page_cursor = crawl_comments(bv, oid, writer, is_second=True, page_cursor=page_cursor)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(f\"Scraping complete! Results saved as {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4128f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape comments for 特朗普实施对等关税，赌你们的枪里没有子弹？_哔哩哔哩_bilibili...\n",
      "Scraping complete! Results saved as 特朗普实施对等关税，赌你们的枪里没有子弹_评论.csv\n"
     ]
    }
   ],
   "source": [
    "#Video 3: Trump Implements Reciprocal Tariffs — Do You Even Have Bullets in Your Guns?\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Securely read cookies ===\n",
    "def get_header(cookie_path='bili_cookie3.txt'):\n",
    "    if not os.path.exists(cookie_path):\n",
    "        raise FileNotFoundError(f\"Cookie file '{cookie_path}' not found. Please check the path.\")\n",
    "    with open(cookie_path, 'r', encoding='utf-8') as f:\n",
    "        cookie = f.read().strip()\n",
    "    headers = {\n",
    "        \"Cookie\": cookie,\n",
    "        \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# === 2. Retrieve video information (OID + title) ===\n",
    "def get_video_info(bv):\n",
    "    url = f\"https://www.bilibili.com/video/{bv}\"\n",
    "    resp = requests.get(url, headers=get_header())\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text\n",
    "    \n",
    "    oid_match = re.search(r'\"aid\":(\\d+),\"bvid\":\"{}\"'.format(bv), text)\n",
    "    title_match = re.search(r'<title.*?>(.*?)</title>', text)\n",
    "    \n",
    "    if not oid_match or not title_match:\n",
    "        raise ValueError(\"Failed to extract OID or Title from the video page.\")\n",
    "    \n",
    "    oid = oid_match.group(1)\n",
    "    title = title_match.group(1).strip().replace('/', '_')  # 防止文件名非法字符\n",
    "    return oid, title\n",
    "\n",
    "# === 3. Scrape comments ===\n",
    "def crawl_comments(bv, oid, csv_writer, is_second=True, page_cursor=''):\n",
    "    mode, plat, type_, web_location = 2, 1, 1, 1315875\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    pagination = {\"offset\": \"\"} if not page_cursor else {\"offset\": json.dumps({\"type\": 3, \"direction\": 1, \"Data\": {\"cursor\": page_cursor}})}\n",
    "    pagination_str = json.dumps(pagination, separators=(',', ':'))\n",
    "    \n",
    "    # 生成w_rid\n",
    "    params = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str)}&plat={plat}&seek_rpid=&type={type_}&web_location={web_location}&wts={timestamp}ea1db124af3c7062474693fa704f4ff8\"\n",
    "    w_rid = hashlib.md5(params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    url = (\n",
    "        f\"https://api.bilibili.com/x/v2/reply/wbi/main?\"\n",
    "        f\"oid={oid}&type={type_}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}\"\n",
    "        f\"&plat={plat}&seek_rpid=&web_location={web_location}&w_rid={w_rid}&wts={timestamp}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(url, headers=get_header())\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    replies = data.get('data', {}).get('replies', [])\n",
    "    if not replies:\n",
    "        return None  # No more replies\n",
    "\n",
    "    count = 0\n",
    "    for reply in replies:\n",
    "        count += 1\n",
    "        write_comment(reply, csv_writer)\n",
    "        \n",
    "        #  If secondary (nested) comments are enabled\n",
    "        if is_second and reply.get('rcount', 0) > 0:\n",
    "            crawl_sub_comments(oid, reply['rpid'], csv_writer)\n",
    "    \n",
    "    next_cursor = data['data']['cursor']['next']\n",
    "    return next_cursor if next_cursor != 0 else None\n",
    "\n",
    "# === 4. Scrape secondary (nested) comments ===\n",
    "def crawl_sub_comments(oid, root_rpid, csv_writer):\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/reply?oid={oid}&type=1&root={root_rpid}&ps=10&pn={page}&web_location=333.788\"\n",
    "        response = requests.get(url, headers=get_header())\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        replies = data.get('data', {}).get('replies', [])\n",
    "        if not replies:\n",
    "            break\n",
    "        \n",
    "        for reply in replies:\n",
    "            write_comment(reply, csv_writer)\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.2)  # Avoid getting banned by scraping too quickly\n",
    "\n",
    "# === 5. Write a single comment to CSV ===\n",
    "def write_comment(reply, csv_writer):\n",
    "    try:\n",
    "        csv_writer.writerow([\n",
    "            reply.get('parent', ''),\n",
    "            reply.get('rpid', ''),\n",
    "            reply.get('mid', ''),\n",
    "            reply['member'].get('uname', ''),\n",
    "            reply['member']['level_info'].get('current_level', ''),\n",
    "            reply['member'].get('sex', ''),\n",
    "            reply['content'].get('message', '').replace('\\n', ' '),\n",
    "            pd.to_datetime(reply['ctime'], unit='s'),\n",
    "            reply['reply_control'].get('sub_reply_entry_text', '0').strip(),\n",
    "            reply.get('like', 0),\n",
    "            reply['member'].get('sign', ''),\n",
    "            reply['reply_control'].get('location', '未知')[5:] if 'location' in reply['reply_control'] else '未知',\n",
    "            '是' if reply['member']['vip'].get('vipStatus', 0) != 0 else '否',\n",
    "            reply['member'].get('avatar', '')\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"写入评论出错: {e}\")\n",
    "\n",
    "# === 6. Main program entry point ===\n",
    "def main():\n",
    "    bv = input(\"请输入B站BV号（如BV1CDdWYHEtU）: \").strip()\n",
    "    oid, title = get_video_info(bv)\n",
    "    output_file = f\"{title[:20]}_评论.csv\"\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['上级评论ID', '评论ID', '用户ID', '用户名', '用户等级', '性别', '评论内容', '评论时间', '回复数', '点赞数', '个性签名', 'IP属地', '是否是大会员', '头像'])\n",
    "        \n",
    "        print(f\"Starting to scrape comments for {title}...\")\n",
    "        page_cursor = ''\n",
    "        while page_cursor is not None:\n",
    "            page_cursor = crawl_comments(bv, oid, writer, is_second=True, page_cursor=page_cursor)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(f\"Scraping complete! Results saved as {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a308125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取 【巫师】全球关税混战背后，是不能输的终局之战_哔哩哔哩_bilibili 的评论...\n",
      "爬取完成！结果保存为 【巫师】全球关税混战背后，是不能输的终局_评论.csv\n"
     ]
    }
   ],
   "source": [
    "#Video 4: [Wu Shi] Behind the Global Tariff Chaos Lies the Final Battle That Must Not Be Lost \n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Securely read cookies ===\n",
    "def get_header(cookie_path='bili_cookie4.txt'):\n",
    "    if not os.path.exists(cookie_path):\n",
    "        raise FileNotFoundError(f\"Cookie file '{cookie_path}' not found. Please check the path.\")\n",
    "    with open(cookie_path, 'r', encoding='utf-8') as f:\n",
    "        cookie = f.read().strip()\n",
    "    headers = {\n",
    "        \"Cookie\": cookie,\n",
    "        \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# === 2. Retrieve video information (OID + title) ===\n",
    "def get_video_info(bv):\n",
    "    url = f\"https://www.bilibili.com/video/{bv}\"\n",
    "    resp = requests.get(url, headers=get_header())\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text\n",
    "    \n",
    "    oid_match = re.search(r'\"aid\":(\\d+),\"bvid\":\"{}\"'.format(bv), text)\n",
    "    title_match = re.search(r'<title.*?>(.*?)</title>', text)\n",
    "    \n",
    "    if not oid_match or not title_match:\n",
    "        raise ValueError(\"Failed to extract OID or Title from the video page.\")\n",
    "    \n",
    "    oid = oid_match.group(1)\n",
    "    title = title_match.group(1).strip().replace('/', '_')  # 防止文件名非法字符\n",
    "    return oid, title\n",
    "\n",
    "# === 3. Scrape comments ===\n",
    "def crawl_comments(bv, oid, csv_writer, is_second=True, page_cursor=''):\n",
    "    mode, plat, type_, web_location = 2, 1, 1, 1315875\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    pagination = {\"offset\": \"\"} if not page_cursor else {\"offset\": json.dumps({\"type\": 3, \"direction\": 1, \"Data\": {\"cursor\": page_cursor}})}\n",
    "    pagination_str = json.dumps(pagination, separators=(',', ':'))\n",
    "    \n",
    "    # Generate w_rid\n",
    "    params = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str)}&plat={plat}&seek_rpid=&type={type_}&web_location={web_location}&wts={timestamp}ea1db124af3c7062474693fa704f4ff8\"\n",
    "    w_rid = hashlib.md5(params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    url = (\n",
    "        f\"https://api.bilibili.com/x/v2/reply/wbi/main?\"\n",
    "        f\"oid={oid}&type={type_}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}\"\n",
    "        f\"&plat={plat}&seek_rpid=&web_location={web_location}&w_rid={w_rid}&wts={timestamp}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(url, headers=get_header())\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    replies = data.get('data', {}).get('replies', [])\n",
    "    if not replies:\n",
    "        return None  # No more replies\n",
    "\n",
    "    count = 0\n",
    "    for reply in replies:\n",
    "        count += 1\n",
    "        write_comment(reply, csv_writer)\n",
    "        \n",
    "        # If secondary (nested) comments are enabled\n",
    "        if is_second and reply.get('rcount', 0) > 0:\n",
    "            crawl_sub_comments(oid, reply['rpid'], csv_writer)\n",
    "    \n",
    "    next_cursor = data['data']['cursor']['next']\n",
    "    return next_cursor if next_cursor != 0 else None\n",
    "\n",
    "# === 4. Scrape secondary (nested) comments ===\n",
    "def crawl_sub_comments(oid, root_rpid, csv_writer):\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/reply?oid={oid}&type=1&root={root_rpid}&ps=10&pn={page}&web_location=333.788\"\n",
    "        response = requests.get(url, headers=get_header())\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        replies = data.get('data', {}).get('replies', [])\n",
    "        if not replies:\n",
    "            break\n",
    "        \n",
    "        for reply in replies:\n",
    "            write_comment(reply, csv_writer)\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.2)  # Avoid getting banned by scraping too quickly\n",
    "\n",
    "# === 5. Write a single comment to CSV ===\n",
    "def write_comment(reply, csv_writer):\n",
    "    try:\n",
    "        csv_writer.writerow([\n",
    "            reply.get('parent', ''),\n",
    "            reply.get('rpid', ''),\n",
    "            reply.get('mid', ''),\n",
    "            reply['member'].get('uname', ''),\n",
    "            reply['member']['level_info'].get('current_level', ''),\n",
    "            reply['member'].get('sex', ''),\n",
    "            reply['content'].get('message', '').replace('\\n', ' '),\n",
    "            pd.to_datetime(reply['ctime'], unit='s'),\n",
    "            reply['reply_control'].get('sub_reply_entry_text', '0').strip(),\n",
    "            reply.get('like', 0),\n",
    "            reply['member'].get('sign', ''),\n",
    "            reply['reply_control'].get('location', '未知')[5:] if 'location' in reply['reply_control'] else '未知',\n",
    "            '是' if reply['member']['vip'].get('vipStatus', 0) != 0 else '否',\n",
    "            reply['member'].get('avatar', '')\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"写入评论出错: {e}\")\n",
    "\n",
    "# === 6. Main program entry point ===\n",
    "def main():\n",
    "    bv = input(\"请输入B站BV号（如BV1CDdWYHEtU）: \").strip()\n",
    "    oid, title = get_video_info(bv)\n",
    "    output_file = f\"{title[:20]}_评论.csv\"\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['上级评论ID', '评论ID', '用户ID', '用户名', '用户等级', '性别', '评论内容', '评论时间', '回复数', '点赞数', '个性签名', 'IP属地', '是否是大会员', '头像'])\n",
    "        \n",
    "        print(f\"开始爬取 {title} 的评论...\")\n",
    "        page_cursor = ''\n",
    "        while page_cursor is not None:\n",
    "            page_cursor = crawl_comments(bv, oid, writer, is_second=True, page_cursor=page_cursor)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(f\"爬取完成！结果保存为 {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f53e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape comments for 【厉害】海湖庄园协议：特朗普疯狂关税背后，最终棋局是什么？_哔哩哔哩_bilibili...\n",
      "Scraping complete! Results saved as 【厉害】海湖庄园协议：特朗普疯狂关税背后_评论.csv\n"
     ]
    }
   ],
   "source": [
    "#Video 5. [Li Hai] Mar-a-Lago Agreement: What Is the Endgame Behind Trump’s Tariff Frenzy?\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Securely read cookies ===\n",
    "def get_header(cookie_path='bili_cookie5.txt'):\n",
    "    if not os.path.exists(cookie_path):\n",
    "        raise FileNotFoundError(f\"Cookie file '{cookie_path}' not found. Please check the path.\")\n",
    "    with open(cookie_path, 'r', encoding='utf-8') as f:\n",
    "        cookie = f.read().strip()\n",
    "    headers = {\n",
    "        \"Cookie\": cookie,\n",
    "        \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# === 2. Retrieve video information (OID + title) ===\n",
    "def get_video_info(bv):\n",
    "    url = f\"https://www.bilibili.com/video/{bv}\"\n",
    "    resp = requests.get(url, headers=get_header())\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text\n",
    "    \n",
    "    oid_match = re.search(r'\"aid\":(\\d+),\"bvid\":\"{}\"'.format(bv), text)\n",
    "    title_match = re.search(r'<title.*?>(.*?)</title>', text)\n",
    "    \n",
    "    if not oid_match or not title_match:\n",
    "        raise ValueError(\"Failed to extract OID or Title from the video page.\")\n",
    "    \n",
    "    oid = oid_match.group(1)\n",
    "    title = title_match.group(1).strip().replace('/', '_')  # 防止文件名非法字符\n",
    "    return oid, title\n",
    "\n",
    "# === 3. Scrape comments ===\n",
    "def crawl_comments(bv, oid, csv_writer, is_second=True, page_cursor=''):\n",
    "    mode, plat, type_, web_location = 2, 1, 1, 1315875\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    pagination = {\"offset\": \"\"} if not page_cursor else {\"offset\": json.dumps({\"type\": 3, \"direction\": 1, \"Data\": {\"cursor\": page_cursor}})}\n",
    "    pagination_str = json.dumps(pagination, separators=(',', ':'))\n",
    "    \n",
    "    #Generate w_rid\n",
    "    params = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str)}&plat={plat}&seek_rpid=&type={type_}&web_location={web_location}&wts={timestamp}ea1db124af3c7062474693fa704f4ff8\"\n",
    "    w_rid = hashlib.md5(params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    url = (\n",
    "        f\"https://api.bilibili.com/x/v2/reply/wbi/main?\"\n",
    "        f\"oid={oid}&type={type_}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}\"\n",
    "        f\"&plat={plat}&seek_rpid=&web_location={web_location}&w_rid={w_rid}&wts={timestamp}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(url, headers=get_header())\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    replies = data.get('data', {}).get('replies', [])\n",
    "    if not replies:\n",
    "        return None  # No more replies\n",
    "\n",
    "    count = 0\n",
    "    for reply in replies:\n",
    "        count += 1\n",
    "        write_comment(reply, csv_writer)\n",
    "        \n",
    "        #  If secondary (nested) comments are enabled\n",
    "        if is_second and reply.get('rcount', 0) > 0:\n",
    "            crawl_sub_comments(oid, reply['rpid'], csv_writer)\n",
    "    \n",
    "    next_cursor = data['data']['cursor']['next']\n",
    "    return next_cursor if next_cursor != 0 else None\n",
    "\n",
    "# === 4. Scrape secondary (nested) comments ===\n",
    "def crawl_sub_comments(oid, root_rpid, csv_writer):\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/reply?oid={oid}&type=1&root={root_rpid}&ps=10&pn={page}&web_location=333.788\"\n",
    "        response = requests.get(url, headers=get_header())\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        replies = data.get('data', {}).get('replies', [])\n",
    "        if not replies:\n",
    "            break\n",
    "        \n",
    "        for reply in replies:\n",
    "            write_comment(reply, csv_writer)\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.2)  # Avoid getting banned by scraping too quickly\n",
    "\n",
    "# === 5. Write a single comment to CSV ===\n",
    "def write_comment(reply, csv_writer):\n",
    "    try:\n",
    "        csv_writer.writerow([\n",
    "            reply.get('parent', ''),\n",
    "            reply.get('rpid', ''),\n",
    "            reply.get('mid', ''),\n",
    "            reply['member'].get('uname', ''),\n",
    "            reply['member']['level_info'].get('current_level', ''),\n",
    "            reply['member'].get('sex', ''),\n",
    "            reply['content'].get('message', '').replace('\\n', ' '),\n",
    "            pd.to_datetime(reply['ctime'], unit='s'),\n",
    "            reply['reply_control'].get('sub_reply_entry_text', '0').strip(),\n",
    "            reply.get('like', 0),\n",
    "            reply['member'].get('sign', ''),\n",
    "            reply['reply_control'].get('location', '未知')[5:] if 'location' in reply['reply_control'] else '未知',\n",
    "            '是' if reply['member']['vip'].get('vipStatus', 0) != 0 else '否',\n",
    "            reply['member'].get('avatar', '')\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"写入评论出错: {e}\")\n",
    "\n",
    "# === 6. Main program entry point ===\n",
    "def main():\n",
    "    bv = input(\"请输入B站BV号（如BV1CDdWYHEtU）: \").strip()\n",
    "    oid, title = get_video_info(bv)\n",
    "    output_file = f\"{title[:20]}_评论.csv\"\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['上级评论ID', '评论ID', '用户ID', '用户名', '用户等级', '性别', '评论内容', '评论时间', '回复数', '点赞数', '个性签名', 'IP属地', '是否是大会员', '头像'])\n",
    "        \n",
    "        print(f\"Starting to scrape comments for {title}...\")\n",
    "        page_cursor = ''\n",
    "        while page_cursor is not None:\n",
    "            page_cursor = crawl_comments(bv, oid, writer, is_second=True, page_cursor=page_cursor)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(f\"Scraping complete! Results saved as {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
